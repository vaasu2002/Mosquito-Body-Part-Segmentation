# -*- coding: utf-8 -*-
"""MaskRCNN_Train_MosquitoDataCustom.ipynb

Automatically generated by Colaboratory.

### **Project:** **Instance Segmentation on Mosquito Dataset using Mask RCNN**

### **Omdena - Vectech**

Author: Srirupa Guha 
(LinkedIn Profile Link: https://www.linkedin.com/in/vaasu-bisht/)

Last Modified Date: June 7,2022

Dataset used: Scale.Rapid annotations on Mosquito Body Parts Dataset

Train, Val Split: 803 images for training, 143 images for validation


Model Used: Mask RCNN by Matterport

Source Code: https://github.com/matterport/Mask_RCNN

Other Relevant References:

https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/coco.py

https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/coco.py

Resources and Source Code for converting annotataed masked images to COCO json format for training Mask RCNN:

https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch

https://github.com/chrise96/image-to-coco-json-converter/blob/master/src/create_annotations.py

https://github.com/chrise96/image-to-coco-json-converter/blob/master/create-custom-coco-dataset.ipynb

Description:

1. Scale Rapid annotated images were converted to COCO json format

2. Matterport's Mask RCNN source code was implemented with necessary changes and the pre-trained Mask RCNN on COCO dataset was trained on the mosquito dataset

3. Model Backbone used: ResNet 101

4. Results:

Mean IoU Score across all images and classes:  0.8973593698245175 (89.7%)

Mean IoU scores for each of the categories are as follows: 

abdomen:  0.9845050230249763 (98.4%)
wing:  0.9373929278192849 (93.7%)
leg:  0.8925182551145554. (89.2%)
thorax:  0.8975720097621282 (89.7%)
head:    0.8450502955786561 (84.5%)
antennae:  0.6311082995456198. (63.1%)

5. Next Step:

To train this model further on more Scale.Rapid annotated images with more data collected for minority classes where IoU scores are low (e.g.: proboscis, antennae etc.) and augmentations

6. Mask RCNN for instance segmentation reference article:

https://towardsdatascience.com/computer-vision-instance-segmentation-with-mask-r-cnn-7983502fcad1
"""

# !pip install tensorflow==1.13.1 
# !pip install keras==2.0.8
# !pip install h5py==2.10.0

import os
import sys
import time
import numpy as np
import imgaug  # https://github.com/aleju/imgaug (pip3 install imgaug)

# Download and install the Python COCO tools from https://github.com/waleedka/coco
# That's a fork from the original https://github.com/pdollar/coco with a bug
# fix for Python 3.
# I submitted a pull request https://github.com/cocodataset/cocoapi/pull/50
# If the PR is merged then use the original repo.
# Note: Edit PythonAPI/Makefile and replace "python" with "python3".
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from pycocotools import mask as maskUtils

import zipfile
import urllib.request
import shutil

# Commented out IPython magic to ensure Python compatibility.
from PIL import Image                                      # (pip install Pillow)
import numpy as np                                         # (pip install numpy)
from skimage import measure                                # (pip install scikit-image)
from shapely.geometry import Polygon, MultiPolygon         # (pip install Shapely)
import os
import json

from google.colab import drive
drive.mount('/content/gdrive')
from google.colab.patches import cv2_imshow

import os
import sys
import random
import math
import re
import time
import numpy as np
import cv2
import matplotlib
import matplotlib.pyplot as plt

import zipfile
import urllib.request
import shutil

# Root directory of the project
ROOT_DIR = os.path.abspath("gdrive/My Drive/Vectech/Mask_RCNN_v4/")

# Import Mask RCNN
sys.path.append(ROOT_DIR)  # To find local version of the library

# %matplotlib inline 

# Import Mask RCNN
sys.path.append(ROOT_DIR)  # To find local version of the library
from mrcnn.config import Config
from mrcnn import model as modellib, utils
from mrcnn import visualize
from mrcnn.model import log

# Path to trained weights file
COCO_MODEL_PATH = os.path.join(ROOT_DIR, "mask_rcnn_coco.h5")

# Directory to save logs and trained model
MODEL_DIR = os.path.join(ROOT_DIR, "logs")

TRAIN_DATASET_DIR = os.path.join(ROOT_DIR, "Data/Scale Data New train and val/Train/Original/")
TRAIN_IMAGE_DIR = os.path.join(ROOT_DIR, "Data/Scale Data New train and val/Train/Original/")

VAL_DATASET_DIR = os.path.join(ROOT_DIR, "Data/Scale Data New train and val/Val/Original/")
VAL_IMAGE_DIR = os.path.join(ROOT_DIR, "Data/Scale Data New train and val/Val/Original/")

# Local path to trained weights file
COCO_MODEL_PATH = os.path.join(ROOT_DIR, "mask_rcnn_coco.h5")
# Download COCO trained weights from Releases if needed
if not os.path.exists(COCO_MODEL_PATH):
    utils.download_trained_weights(COCO_MODEL_PATH)

############################################################
#  Configurations
############################################################

class CocoConfig(Config):
    """Configuration for training on the toy shapes dataset.
    Derives from the base Config class and overrides values specific
    to the toy shapes dataset.
    """
    # Give the configuration a recognizable name
    NAME = "coco"

    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each
    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).
    GPU_COUNT = 1
    IMAGES_PER_GPU = 8

    # Number of classes (including background)
    NUM_CLASSES = 1 + 8  # background + 8 shapes

    # Use small images for faster training. Set the limits of the small side
    # the large side, and that determines the image shape.
    IMAGE_MIN_DIM = 128
    IMAGE_MAX_DIM = 128

    # Use smaller anchors because our image and objects are small
    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels

    # Reduce training ROIs per image because the images are small and have
    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.
    TRAIN_ROIS_PER_IMAGE = 32

    # Use a small epoch since the data is simple
    STEPS_PER_EPOCH = 100

    # use small validation steps since the epoch is small
    VALIDATION_STEPS = 5
    
config = CocoConfig()
config.display()

def get_ax(rows=1, cols=1, size=8):
    """Return a Matplotlib Axes array to be used in
    all visualizations in the notebook. Provide a
    central point to control graph sizes.
    
    Change the default size attribute to control the size
    of rendered images
    """
    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))
    return ax

############################################################
#  Dataset
############################################################

class CocoDataset(utils.Dataset):
    def load_coco_train(self):
        """Load a subset of the COCO dataset.
        dataset_dir: The root directory of the COCO dataset.
        subset: What to load (train, val, minival, valminusminival)
        year: What dataset year to load (2014, 2017) as a string, not an integer
        class_ids: If provided, only loads images that have the given classes.
        class_map: TODO: Not implemented yet. Supports maping classes from
            different datasets to the same class ID.
        return_coco: If True, returns the COCO object.
        auto_download: Automatically download and unzip MS-COCO images and annotations
        """

        annotations = json.load(open('gdrive/My Drive/Vectech/Mask_RCNN_v4/Data/Scale Data New train and val/Train/Original/annotations.json'))

        # Load all class ids
        categories_list = annotations['categories']

        category_ids = []

        for category in categories_list:
          cat_id = category['id']
          category_ids.append(cat_id)

        class_ids = sorted(category_ids)

        # Load all image ids
        annotations_list = annotations['annotations']

        image_id_list = []

        for annotation in annotations_list:
          image_id = annotation['image_id']
          image_id_list.append(image_id)

        image_ids = sorted(set(image_id_list))

        # Add classe names

        for category in annotations['categories']:
          class_name = category['name']
          class_id = category['id']
          self.add_class("coco", class_id, class_name)

        # Add images
        for i in image_ids:
            self.add_image(
                "coco", image_id=i,
                path=os.path.join(TRAIN_IMAGE_DIR, annotations['images'][i]["file_name"]),
                width=annotations['images'][i]["width"],
                height=annotations['images'][i]["height"],
                annotations=annotations['annotations'][i])
            
    def load_coco_val(self):
        """Load a subset of the COCO dataset.
        dataset_dir: The root directory of the COCO dataset.
        subset: What to load (train, val, minival, valminusminival)
        year: What dataset year to load (2014, 2017) as a string, not an integer
        class_ids: If provided, only loads images that have the given classes.
        class_map: TODO: Not implemented yet. Supports maping classes from
            different datasets to the same class ID.
        return_coco: If True, returns the COCO object.
        auto_download: Automatically download and unzip MS-COCO images and annotations
        """

        annotations = json.load(open('gdrive/My Drive/Vectech/Mask_RCNN_v4/Data/Scale Data New train and val/Val/Original/annotations.json'))

        # Load all class ids
        categories_list = annotations['categories']

        category_ids = []

        for category in categories_list:
          cat_id = category['id']
          category_ids.append(cat_id)

        class_ids = sorted(category_ids)

        # Load all image ids
        annotations_list = annotations['annotations']

        image_id_list = []

        for annotation in annotations_list:
          image_id = annotation['image_id']
          image_id_list.append(image_id)

        image_ids = sorted(set(image_id_list))

        # Add classe names

        for category in annotations['categories']:
          class_name = category['name']
          class_id = category['id']
          self.add_class("coco", class_id, class_name)

        # Add images
        for i in image_ids:
            self.add_image(
                "coco", image_id=i,
                path=os.path.join(VAL_IMAGE_DIR, annotations['images'][i]["file_name"]),
                width=annotations['images'][i]["width"],
                height=annotations['images'][i]["height"],
                annotations=annotations['annotations'][i])
            
    def auto_download(self, dataDir, dataType, dataYear):
        """Download the COCO dataset/annotations if requested.
        dataDir: The root directory of the COCO dataset.
        dataType: What to load (train, val, minival, valminusminival)
        dataYear: What dataset year to load (2014, 2017) as a string, not an integer
        Note:
            For 2014, use "train", "val", "minival", or "valminusminival"
            For 2017, only "train" and "val" annotations are available
        """

        # Setup paths and file names
        if dataType == "minival" or dataType == "valminusminival":
            imgDir = "{}/{}{}".format(dataDir, "val", dataYear)
            imgZipFile = "{}/{}{}.zip".format(dataDir, "val", dataYear)
            imgURL = "http://images.cocodataset.org/zips/{}{}.zip".format("val", dataYear)
        else:
            imgDir = "{}/{}{}".format(dataDir, dataType, dataYear)
            imgZipFile = "{}/{}{}.zip".format(dataDir, dataType, dataYear)
            imgURL = "http://images.cocodataset.org/zips/{}{}.zip".format(dataType, dataYear)
        # print("Image paths:"); print(imgDir); print(imgZipFile); print(imgURL)

        # Create main folder if it doesn't exist yet
        if not os.path.exists(dataDir):
            os.makedirs(dataDir)

        # Download images if not available locally
        if not os.path.exists(imgDir):
            os.makedirs(imgDir)
            print("Downloading images to " + imgZipFile + " ...")
            with urllib.request.urlopen(imgURL) as resp, open(imgZipFile, 'wb') as out:
                shutil.copyfileobj(resp, out)
            print("... done downloading.")
            print("Unzipping " + imgZipFile)
            with zipfile.ZipFile(imgZipFile, "r") as zip_ref:
                zip_ref.extractall(dataDir)
            print("... done unzipping")
        print("Will use images in " + imgDir)

        # Setup annotations data paths
        annDir = "{}/annotations".format(dataDir)
        if dataType == "minival":
            annZipFile = "{}/instances_minival2014.json.zip".format(dataDir)
            annFile = "{}/instances_minival2014.json".format(annDir)
            annURL = "https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?dl=0"
            unZipDir = annDir
        elif dataType == "valminusminival":
            annZipFile = "{}/instances_valminusminival2014.json.zip".format(dataDir)
            annFile = "{}/instances_valminusminival2014.json".format(annDir)
            annURL = "https://dl.dropboxusercontent.com/s/s3tw5zcg7395368/instances_valminusminival2014.json.zip?dl=0"
            unZipDir = annDir
        else:
            annZipFile = "{}/annotations_trainval{}.zip".format(dataDir, dataYear)
            annFile = "{}/instances_{}{}.json".format(annDir, dataType, dataYear)
            annURL = "http://images.cocodataset.org/annotations/annotations_trainval{}.zip".format(dataYear)
            unZipDir = dataDir
        # print("Annotations paths:"); print(annDir); print(annFile); print(annZipFile); print(annURL)

        # Download annotations if not available locally
        if not os.path.exists(annDir):
            os.makedirs(annDir)
        if not os.path.exists(annFile):
            if not os.path.exists(annZipFile):
                print("Downloading zipped annotations to " + annZipFile + " ...")
                with urllib.request.urlopen(annURL) as resp, open(annZipFile, 'wb') as out:
                    shutil.copyfileobj(resp, out)
                print("... done downloading.")
            print("Unzipping " + annZipFile)
            with zipfile.ZipFile(annZipFile, "r") as zip_ref:
                zip_ref.extractall(unZipDir)
            print("... done unzipping")
        print("Will use annotations in " + annFile)

    def load_mask(self, image_id):
        """Load instance masks for the given image.
        Different datasets use different ways to store masks. This
        function converts the different mask format to one format
        in the form of a bitmap [height, width, instances].
        Returns:
        masks: A bool array of shape [height, width, instance count] with
            one mask per instance.
        class_ids: a 1D array of class IDs of the instance masks.
        """
        image_info = image_id 

        instance_masks = []
        class_ids = []

        annotations_file = json.load(open('gdrive/My Drive/Vectech/Mask_RCNN_v4/Data/Scale Data New train and val/Val/Original/annotations.json'))
      
        annot_list = []

        for annot in annotations_file['annotations']:
          if annot['image_id'] == image_id:
            annot_list.append(annot)
          else:
            continue

        annotations = annot_list

        for image in annotations_file['images']:
          if image['id'] == image_id:
            height = image['height']
            width = image['width']

        # Build mask of shape [height, width, instance_count] and list
        # of class IDs that correspond to each channel of the mask.
        for annotation in annotations:
            class_id = annotation['category_id']
            if class_id:
                m = self.annToMask(annotation, height, width)
                # Some objects are so small that they're less than 1 pixel area
                # and end up rounded out. Skip those objects.
                # if m.max() < 1:
                #     continue
                # # Is it a crowd? If so, use a negative class ID.
                # if annotation['iscrowd']:
                #     # Use negative class ID for crowds
                #     class_id *= -1
                #     # For crowd masks, annToMask() sometimes returns a mask
                #     # smaller than the given dimensions. If so, resize it.
                #     if m.shape[0] != height or m.shape[1] != width:
                #         m = np.ones([height, width], dtype=bool)
                instance_masks.append(m)
                class_ids.append(class_id)

        # Pack instance masks into an array
        if class_ids:
            mask = np.stack(instance_masks, axis=2).astype(np.bool)
            class_ids = np.array(class_ids, dtype=np.int32)
            return mask, class_ids
        else:
            # Call super class to return an empty mask
            return super(CocoDataset, self).load_mask(image_id)

    def image_reference(self, image_id):
        """Return a link to the image in the COCO Website."""
        info = self.image_info[image_id]
        return ('Image id: ', + info)

    # The following two functions are from pycocotools with a few changes.

    def annToRLE(self, ann, height, width):
        """
        Convert annotation which can be polygons, uncompressed RLE to RLE.
        :return: binary mask (numpy 2D array)
        """
        segm = ann['segmentation']
        if isinstance(segm, list):
            # polygon -- a single object might consist of multiple parts
            # we merge all parts into one mask rle code
            rles = maskUtils.frPyObjects(segm, height, width)
            rle = maskUtils.merge(rles)
        elif isinstance(segm['counts'], list):
            # uncompressed RLE
            rle = maskUtils.frPyObjects(segm, height, width)
        else:
            # rle
            rle = ann['segmentation']
        return rle

    def annToMask(self, ann, height, width):
        """
        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.
        :return: binary mask (numpy 2D array)
        """
        rle = self.annToRLE(ann, height, width)
        m = maskUtils.decode(rle)
        return m

############################################################
#  COCO Evaluation
############################################################

def build_coco_results(dataset, image_ids, rois, class_ids, scores, masks):
    """Arrange resutls to match COCO specs in http://cocodataset.org/#format
    """
    # If no results, return an empty list
    if rois is None:
        return []

    results = []
    for image_id in image_ids:
        # Loop through detections
        for i in range(rois.shape[0]):
            class_id = class_ids[i]
            score = scores[i]
            bbox = np.around(rois[i], 1)
            mask = masks[:, :, i]

            result = {
                "image_id": image_id,
                "category_id": dataset.get_source_class_id(class_id, "coco"),
                "bbox": [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]],
                "score": score,
                "segmentation": maskUtils.encode(np.asfortranarray(mask))
            }
            results.append(result)
    return results


def evaluate_coco(model, dataset, coco, eval_type="bbox", limit=0, image_ids=None):
    """Runs official COCO evaluation.
    dataset: A Dataset object with valiadtion data
    eval_type: "bbox" or "segm" for bounding box or segmentation evaluation
    limit: if not 0, it's the number of images to use for evaluation
    """
    # Pick COCO images from the dataset
    image_ids = image_ids or dataset.image_ids

    # Limit to a subset
    if limit:
        image_ids = image_ids[:limit]

    # Get corresponding COCO image IDs.
    coco_image_ids = [dataset.image_info[id]["id"] for id in image_ids]

    t_prediction = 0
    t_start = time.time()

    results = []
    for i, image_id in enumerate(image_ids):
        # Load image
        image = dataset.load_image(image_id)

        # Run detection
        t = time.time()
        r = model.detect([image], verbose=0)[0]
        t_prediction += (time.time() - t)

        # Convert results to COCO format
        # Cast masks to uint8 because COCO tools errors out on bool
        image_results = build_coco_results(dataset, coco_image_ids[i:i + 1],
                                           r["rois"], r["class_ids"],
                                           r["scores"],
                                           r["masks"].astype(np.uint8))
        results.extend(image_results)

    # Load results. This modifies results with additional attributes.
    coco_results = loadRes(dataset, results)

    # Evaluate
    cocoEval = COCOeval(coco, coco_results, eval_type)
    cocoEval.params.imgIds = coco_image_ids
    cocoEval.evaluate()
    cocoEval.accumulate()
    cocoEval.summarize()

    print("Prediction time: {}. Average {}/image".format(
        t_prediction, t_prediction / len(image_ids)))
    print("Total time: ", time.time() - t_start)

def createIndex(val_annotations_file_path):
    # create index
    print('creating index...')
    anns, cats, imgs = {}, {}, {}
    imgToAnns,catToImgs = defaultdict(list),defaultdict(list)

    dataset = json.load(open(val_annotations_file_path))

    if 'annotations' in dataset:
        for ann in dataset['annotations']:
            imgToAnns[ann['image_id']].append(ann)
            anns[ann['id']] = ann

    if 'images' in dataset:
        for img in dataset['images']:
            imgs[img['id']] = img

    if 'categories' in dataset:
        for cat in dataset['categories']:
            cats[cat['id']] = cat

    if 'annotations' in dataset and 'categories' in dataset:
        for ann in dataset['annotations']:
            catToImgs[ann['category_id']].append(ann['image_id'])

    print('index created!')

    return anns, cats, imgToAnns, catToImgs, imgs

def info(dataset):
    """
    Print information about the annotation file.
    :return:
    """
    for key, value in dataset['info'].items():
        print('{}: {}'.format(key, value))

def getAnnIds(dataset, imgIds=[], catIds=[], areaRng=[], iscrowd=None):
    """
    Get ann ids that satisfy given filter conditions. default skips that filter
    :param imgIds  (int array)     : get anns for given imgs
            catIds  (int array)     : get anns for given cats
            areaRng (float array)   : get anns for given area range (e.g. [0 inf])
            iscrowd (boolean)       : get anns for given crowd label (False or True)
    :return: ids (int array)       : integer array of ann ids
    """

    anns, cats, imgToAnns, catToImgs, imgs = createIndex('gdrive/My Drive/Vectech/Mask_RCNN_v4/Data/Scale Data New train and val/Val/Original/annotations.json')

    imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]
    catIds = catIds if _isArrayLike(catIds) else [catIds]

    if len(imgIds) == len(catIds) == len(areaRng) == 0:
        anns = dataset['annotations']
    else:
        if not len(imgIds) == 0:
            lists = [imgToAnns[imgId] for imgId in imgIds if imgId in imgToAnns]
            anns = list(itertools.chain.from_iterable(lists))
        else:
            anns = dataset['annotations']
        anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann['category_id'] in catIds]
        anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]
    if not iscrowd == None:
        ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]
    else:
        ids = [ann['id'] for ann in anns]
    return ids

def getCatIds(dataset, catNms=[], supNms=[], catIds=[]):
    """
    filtering parameters. default skips that filter.
    :param catNms (str array)  : get cats for given cat names
    :param supNms (str array)  : get cats for given supercategory names
    :param catIds (int array)  : get cats for given cat ids
    :return: ids (int array)   : integer array of cat ids
    """
    catNms = catNms if _isArrayLike(catNms) else [catNms]
    supNms = supNms if _isArrayLike(supNms) else [supNms]
    catIds = catIds if _isArrayLike(catIds) else [catIds]

    if len(catNms) == len(supNms) == len(catIds) == 0:
        cats = dataset['categories']
    else:
        cats = dataset['categories']
        cats = cats if len(catNms) == 0 else [cat for cat in cats if cat['name']          in catNms]
        cats = cats if len(supNms) == 0 else [cat for cat in cats if cat['supercategory'] in supNms]
        cats = cats if len(catIds) == 0 else [cat for cat in cats if cat['id']            in catIds]
    ids = [cat['id'] for cat in cats]
    return ids

def getImgIds(dataset, imgIds=[], catIds=[]):
    '''
    Get img ids that satisfy given filter conditions.
    :param imgIds (int array) : get imgs for given ids
    :param catIds (int array) : get imgs with all given cats
    :return: ids (int array)  : integer array of img ids
    '''

    anns, cats, imgToAnns, catToImgs, imgs = createIndex('gdrive/My Drive/Vectech/Mask_RCNN_v4/Data/Scale Data New train and val/Val/Original/annotations.json')

    imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]
    catIds = catIds if _isArrayLike(catIds) else [catIds]

    if len(imgIds) == len(catIds) == 0:
        ids = imgs.keys()
    else:
        ids = set(imgIds)
        for i, catId in enumerate(catIds):
            if i == 0 and len(ids) == 0:
                ids = set(catToImgs[catId])
            else:
                ids &= set(catToImgs[catId])
    return list(ids)

def loadAnns(dataset, ids=[]):
    """
    Load anns with the specified ids.
    :param ids (int array)       : integer ids specifying anns
    :return: anns (object array) : loaded ann objects
    """

    anns, cats, imgToAnns, catToImgs, imgs = createIndex('gdrive/My Drive/Vectech/Mask_RCNN_v4/Data/Scale Data New train and val/Val/Original/annotations.json')

    if _isArrayLike(ids):
        return [anns[id] for id in ids]
    elif type(ids) == int:
        return [anns[ids]]

def loadCats(dataset, ids=[]):
    """
    Load cats with the specified ids.
    :param ids (int array)       : integer ids specifying cats
    :return: cats (object array) : loaded cat objects
    """

    anns, cats, imgToAnns, catToImgs, imgs = createIndex('gdrive/My Drive/Vectech/Mask_RCNN_v4/Data/Scale Data New train and val/Val/Original/annotations.json')

    if _isArrayLike(ids):
        return [cats[id] for id in ids]
    elif type(ids) == int:
        return [cats[ids]]

def loadImgs(dataset, ids=[]):
    """
    Load anns with the specified ids.
    :param ids (int array)       : integer ids specifying img
    :return: imgs (object array) : loaded img objects
    """

    anns, cats, imgToAnns, catToImgs, imgs = createIndex('gdrive/My Drive/Vectech/Mask_RCNN_v4/Data/Scale Data New train and val/Val/Original/annotations.json')

    if _isArrayLike(ids):
        return [imgs[id] for id in ids]
    elif type(ids) == int:
        return [imgs[ids]]

def showAnns(dataset, anns, draw_bbox=False):
    """
    Display the specified annotations.
    :param anns (array of object): annotations to display
    :return: None
    """

    anns, cats, imgToAnns, catToImgs, imgs = createIndex('gdrive/My Drive/Vectech/Mask_RCNN_v4/Data/Scale Data New train and val/Val/Original/annotations.json')

    if len(anns) == 0:
        return 0
    if 'segmentation' in anns[0] or 'keypoints' in anns[0]:
        datasetType = 'instances'
    elif 'caption' in anns[0]:
        datasetType = 'captions'
    else:
        raise Exception('datasetType not supported')
    if datasetType == 'instances':
        ax = plt.gca()
        ax.set_autoscale_on(False)
        polygons = []
        color = []
        for ann in anns:
            c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]
            if 'segmentation' in ann:
                if type(ann['segmentation']) == list:
                    # polygon
                    for seg in ann['segmentation']:
                        poly = np.array(seg).reshape((int(len(seg)/2), 2))
                        polygons.append(Polygon(poly))
                        color.append(c)
                else:
                    # mask
                    t = imgs[ann['image_id']]
                    if type(ann['segmentation']['counts']) == list:
                        rle = maskUtils.frPyObjects([ann['segmentation']], t['height'], t['width'])
                    else:
                        rle = [ann['segmentation']]
                    m = maskUtils.decode(rle)
                    img = np.ones( (m.shape[0], m.shape[1], 3) )
                    if ann['iscrowd'] == 1:
                        color_mask = np.array([2.0,166.0,101.0])/255
                    if ann['iscrowd'] == 0:
                        color_mask = np.random.random((1, 3)).tolist()[0]
                    for i in range(3):
                        img[:,:,i] = color_mask[i]
                    ax.imshow(np.dstack( (img, m*0.5) ))
            if 'keypoints' in ann and type(ann['keypoints']) == list:
                # turn skeleton into zero-based index
                sks = np.array(loadCats(ann['category_id'])[0]['skeleton'])-1
                kp = np.array(ann['keypoints'])
                x = kp[0::3]
                y = kp[1::3]
                v = kp[2::3]
                for sk in sks:
                    if np.all(v[sk]>0):
                        plt.plot(x[sk],y[sk], linewidth=3, color=c)
                plt.plot(x[v>0], y[v>0],'o',markersize=8, markerfacecolor=c, markeredgecolor='k',markeredgewidth=2)
                plt.plot(x[v>1], y[v>1],'o',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)

            if draw_bbox:
                [bbox_x, bbox_y, bbox_w, bbox_h] = ann['bbox']
                poly = [[bbox_x, bbox_y], [bbox_x, bbox_y+bbox_h], [bbox_x+bbox_w, bbox_y+bbox_h], [bbox_x+bbox_w, bbox_y]]
                np_poly = np.array(poly).reshape((4,2))
                polygons.append(Polygon(np_poly))
                color.append(c)

        p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)
        ax.add_collection(p)
        p = PatchCollection(polygons, facecolor='none', edgecolors=color, linewidths=2)
        ax.add_collection(p)
    elif datasetType == 'captions':
        for ann in anns:
            print(ann['caption'])

def loadRes(dataset, resFile):
    """
    Load result file and return a result api object.
    :param   resFile (str)     : file name of result file
    :return: res (obj)         : result api object
    """

    anns, cats, imgToAnns, catToImgs, imgs = createIndex(dataset)
    
    res = COCO()
    res.dataset['images'] = [img for img in dataset['images']]

    print('Loading and preparing results...')
    tic = time.time()
    if type(resFile) == str or (PYTHON_VERSION == 2 and type(resFile) == unicode):
        anns = json.load(open(resFile))
    elif type(resFile) == np.ndarray:
        anns = loadNumpyAnnotations(dataset, resFile)
    else:
        anns = resFile
    assert type(anns) == list, 'results in not an array of objects'
    annsImgIds = [ann['image_id'] for ann in anns]
    assert set(annsImgIds) == (set(annsImgIds) & set(getImgIds())), \
            'Results do not correspond to current coco set'
    if 'caption' in anns[0]:
        imgIds = set([img['id'] for img in res.dataset['images']]) & set([ann['image_id'] for ann in anns])
        res.dataset['images'] = [img for img in res.dataset['images'] if img['id'] in imgIds]
        for id, ann in enumerate(anns):
            ann['id'] = id+1
    elif 'bbox' in anns[0] and not anns[0]['bbox'] == []:
        res.dataset['categories'] = copy.deepcopy(dataset['categories'])
        for id, ann in enumerate(anns):
            bb = ann['bbox']
            x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]
            if not 'segmentation' in ann:
                ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]
            ann['area'] = bb[2]*bb[3]
            ann['id'] = id+1
            ann['iscrowd'] = 0
    elif 'segmentation' in anns[0]:
        res.dataset['categories'] = copy.deepcopy(dataset['categories'])
        for id, ann in enumerate(anns):
            # now only support compressed RLE format as segmentation results
            ann['area'] = maskUtils.area(ann['segmentation'])
            if not 'bbox' in ann:
                ann['bbox'] = maskUtils.toBbox(ann['segmentation'])
            ann['id'] = id+1
            ann['iscrowd'] = 0
    elif 'keypoints' in anns[0]:
        res.dataset['categories'] = copy.deepcopy(dataset['categories'])
        for id, ann in enumerate(anns):
            s = ann['keypoints']
            x = s[0::3]
            y = s[1::3]
            x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)
            ann['area'] = (x1-x0)*(y1-y0)
            ann['id'] = id + 1
            ann['bbox'] = [x0,y0,x1-x0,y1-y0]
    print('DONE (t={:0.2f}s)'.format(time.time()- tic))

    res.dataset['annotations'] = anns
    res.createIndex('gdrive/My Drive/Omdena-Vectech/Mask_RCNN_v4/Mask_RCNN-master/Data/Scale Data New train and val/Val/Original/annotations.json')
    return res

def download(dataset, tarDir = None, imgIds = [] ):
    '''
    Download COCO images from mscoco.org server.
    :param tarDir (str): COCO results directory name
            imgIds (list): images to be downloaded
    :return:
    '''

    anns, cats, imgToAnns, catToImgs, imgs = createIndex('gdrive/My Drive/Vectech/Mask_RCNN_v4/Data/Scale Data New train and val/Val/Original/annotations.json')

    if tarDir is None:
        print('Please specify target directory')
        return -1
    if len(imgIds) == 0:
        imgs = imgs.values()
    else:
        imgs = loadImgs(imgIds)
    N = len(imgs)
    if not os.path.exists(tarDir):
        os.makedirs(tarDir)
    for i, img in enumerate(imgs):
        tic = time.time()
        fname = os.path.join(tarDir, img['file_name'])
        if not os.path.exists(fname):
            urlretrieve(img['coco_url'], fname)
        print('downloaded {}/{} images (t={:0.1f}s)'.format(i, N, time.time()- tic))

def loadNumpyAnnotations(dataset, data):
    """
    Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}
    :param  data (numpy.ndarray)
    :return: annotations (python nested list)
    """

    anns, cats, imgToAnns, catToImgs, imgs = createIndex('gdrive/My Drive/Vectech/Mask_RCNN_v4/Data/Scale Data New train and val/Val/Original/annotations.json')

    print('Converting ndarray to lists...')
    assert(type(data) == np.ndarray)
    print(data.shape)
    assert(data.shape[1] == 7)
    N = data.shape[0]
    ann = []
    for i in range(N):
        if i % 1000000 == 0:
            print('{}/{}'.format(i,N))
        ann += [{
            'image_id'  : int(data[i, 0]),
            'bbox'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],
            'score' : data[i, 5],
            'category_id': int(data[i, 6]),
            }]
    return ann

# Training dataset
dataset_train = CocoDataset()
dataset_train.load_coco_train()
dataset_train.prepare()

# Validation dataset
dataset_val = CocoDataset()
dataset_val.load_coco_val()
dataset_val.prepare()

# Image Augmentation
# Right/Left flip 50% of the time
augmentation = imgaug.augmenters.Fliplr(0.5)

model = modellib.MaskRCNN(mode="training", config=config,model_dir="gdrive/My Drive/Vectech/Mask_RCNN_v4/logs/")

model_path = COCO_MODEL_PATH

# Load weights
print("Loading weights ", model_path)
model.load_weights(model_path, by_name=True, exclude=[ "mrcnn_class_logits", "mrcnn_bbox_fc", "mrcnn_bbox", "mrcnn_mask"])

# *** This training schedule is an example. Update to your needs ***

# Training - Stage 1
print("Training network heads")
model.train(dataset_train, dataset_val,
            learning_rate=config.LEARNING_RATE,
            epochs=5,
            layers='heads',
            augmentation=augmentation)

# Training - Stage 2
# Finetune layers from ResNet stage 4 and up
print("Fine tune Resnet stage 4 and up")
model.train(dataset_train, dataset_val,
            learning_rate=config.LEARNING_RATE,
            epochs=10,
            layers='4+',
            augmentation=augmentation)

# Training - Stage 3
# Fine tune all layers
print("Fine tune all layers")
model.train(dataset_train, dataset_val,
            learning_rate=config.LEARNING_RATE / 10,
            epochs=5,
            layers='all',
            augmentation=augmentation)

model_path = os.path.join(MODEL_DIR, 'FullyTrained_MRCNN_ModelOnMosquitoDataset.h5')
model.keras_model.save_weights(model_path)

class InferenceConfig(CocoConfig):
    # Set batch size to 1 since we'll be running inference on
    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1
    DETECTION_MIN_CONFIDENCE = 0

config = InferenceConfig()
    
config.display()

# Create model object in inference mode.
model = modellib.MaskRCNN(mode="inference", model_dir=MODEL_DIR, config=config)

# Load weights trained on MS-COCO
TRAINED_MRCNN_MODEL = model_path
model.load_weights(TRAINED_MRCNN_MODEL, by_name=True)

class COCO:
    def __init__(self, annotation_file=None):
        """
        Constructor of Microsoft COCO helper class for reading and visualizing annotations.
        :param annotation_file (str): location of annotation file
        :param image_folder (str): location to the folder that hosts images.
        :return:
        """
        # load dataset
        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()
        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)
        if not annotation_file == None:
            print('loading annotations into memory...')
            tic = time.time()
            dataset = json.load(open(annotation_file, 'r'))
            assert type(dataset)==dict, 'annotation file format {} not supported'.format(type(dataset))
            print('Done (t={:0.2f}s)'.format(time.time()- tic))
            self.dataset = dataset
            self.createIndex()

    def createIndex(self):
        # create index
        print('creating index...')
        anns, cats, imgs = {}, {}, {}
        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)
        if 'annotations' in self.dataset:
            for ann in self.dataset['annotations']:
                imgToAnns[ann['image_id']].append(ann)
                anns[ann['id']] = ann

        if 'images' in self.dataset:
            for img in self.dataset['images']:
                imgs[img['id']] = img

        if 'categories' in self.dataset:
            for cat in self.dataset['categories']:
                cats[cat['id']] = cat

        if 'annotations' in self.dataset and 'categories' in self.dataset:
            for ann in self.dataset['annotations']:
                catToImgs[ann['category_id']].append(ann['image_id'])

        print('index created!')

        # create class members
        self.anns = anns
        self.imgToAnns = imgToAnns
        self.catToImgs = catToImgs
        self.imgs = imgs
        self.cats = cats

    def info(self):
        """
        Print information about the annotation file.
        :return:
        """
        for key, value in self.dataset['info'].items():
            print('{}: {}'.format(key, value))

    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):
        """
        Get ann ids that satisfy given filter conditions. default skips that filter
        :param imgIds  (int array)     : get anns for given imgs
               catIds  (int array)     : get anns for given cats
               areaRng (float array)   : get anns for given area range (e.g. [0 inf])
               iscrowd (boolean)       : get anns for given crowd label (False or True)
        :return: ids (int array)       : integer array of ann ids
        """
        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]
        catIds = catIds if _isArrayLike(catIds) else [catIds]

        if len(imgIds) == len(catIds) == len(areaRng) == 0:
            anns = self.dataset['annotations']
        else:
            if not len(imgIds) == 0:
                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]
                anns = list(itertools.chain.from_iterable(lists))
            else:
                anns = self.dataset['annotations']
            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann['category_id'] in catIds]
            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann['area'] > areaRng[0] and ann['area'] < areaRng[1]]
        if not iscrowd == None:
            ids = [ann['id'] for ann in anns if ann['iscrowd'] == iscrowd]
        else:
            ids = [ann['id'] for ann in anns]
        return ids

    def getCatIds(self, catNms=[], supNms=[], catIds=[]):
        """
        filtering parameters. default skips that filter.
        :param catNms (str array)  : get cats for given cat names
        :param supNms (str array)  : get cats for given supercategory names
        :param catIds (int array)  : get cats for given cat ids
        :return: ids (int array)   : integer array of cat ids
        """
        catNms = catNms if _isArrayLike(catNms) else [catNms]
        supNms = supNms if _isArrayLike(supNms) else [supNms]
        catIds = catIds if _isArrayLike(catIds) else [catIds]

        if len(catNms) == len(supNms) == len(catIds) == 0:
            cats = self.dataset['categories']
        else:
            cats = self.dataset['categories']
            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat['name']          in catNms]
            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat['supercategory'] in supNms]
            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat['id']            in catIds]
        ids = [cat['id'] for cat in cats]
        return ids

    def getImgIds(self, imgIds=[], catIds=[]):
        '''
        Get img ids that satisfy given filter conditions.
        :param imgIds (int array) : get imgs for given ids
        :param catIds (int array) : get imgs with all given cats
        :return: ids (int array)  : integer array of img ids
        '''
        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]
        catIds = catIds if _isArrayLike(catIds) else [catIds]

        if len(imgIds) == len(catIds) == 0:
            ids = self.imgs.keys()
        else:
            ids = set(imgIds)
            for i, catId in enumerate(catIds):
                if i == 0 and len(ids) == 0:
                    ids = set(self.catToImgs[catId])
                else:
                    ids &= set(self.catToImgs[catId])
        return list(ids)

    def loadAnns(self, ids=[]):
        """
        Load anns with the specified ids.
        :param ids (int array)       : integer ids specifying anns
        :return: anns (object array) : loaded ann objects
        """
        if _isArrayLike(ids):
            return [self.anns[id] for id in ids]
        elif type(ids) == int:
            return [self.anns[ids]]

    def loadCats(self, ids=[]):
        """
        Load cats with the specified ids.
        :param ids (int array)       : integer ids specifying cats
        :return: cats (object array) : loaded cat objects
        """
        if _isArrayLike(ids):
            return [self.cats[id] for id in ids]
        elif type(ids) == int:
            return [self.cats[ids]]

    def loadImgs(self, ids=[]):
        """
        Load anns with the specified ids.
        :param ids (int array)       : integer ids specifying img
        :return: imgs (object array) : loaded img objects
        """
        if _isArrayLike(ids):
            return [self.imgs[id] for id in ids]
        elif type(ids) == int:
            return [self.imgs[ids]]

    def showAnns(self, anns, draw_bbox=False):
        """
        Display the specified annotations.
        :param anns (array of object): annotations to display
        :return: None
        """
        if len(anns) == 0:
            return 0
        if 'segmentation' in anns[0] or 'keypoints' in anns[0]:
            datasetType = 'instances'
        elif 'caption' in anns[0]:
            datasetType = 'captions'
        else:
            raise Exception('datasetType not supported')
        if datasetType == 'instances':
            ax = plt.gca()
            ax.set_autoscale_on(False)
            polygons = []
            color = []
            for ann in anns:
                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]
                if 'segmentation' in ann:
                    if type(ann['segmentation']) == list:
                        # polygon
                        for seg in ann['segmentation']:
                            poly = np.array(seg).reshape((int(len(seg)/2), 2))
                            polygons.append(Polygon(poly))
                            color.append(c)
                    else:
                        # mask
                        t = self.imgs[ann['image_id']]
                        if type(ann['segmentation']['counts']) == list:
                            rle = maskUtils.frPyObjects([ann['segmentation']], t['height'], t['width'])
                        else:
                            rle = [ann['segmentation']]
                        m = maskUtils.decode(rle)
                        img = np.ones( (m.shape[0], m.shape[1], 3) )
                        if ann['iscrowd'] == 1:
                            color_mask = np.array([2.0,166.0,101.0])/255
                        if ann['iscrowd'] == 0:
                            color_mask = np.random.random((1, 3)).tolist()[0]
                        for i in range(3):
                            img[:,:,i] = color_mask[i]
                        ax.imshow(np.dstack( (img, m*0.5) ))
                if 'keypoints' in ann and type(ann['keypoints']) == list:
                    # turn skeleton into zero-based index
                    sks = np.array(self.loadCats(ann['category_id'])[0]['skeleton'])-1
                    kp = np.array(ann['keypoints'])
                    x = kp[0::3]
                    y = kp[1::3]
                    v = kp[2::3]
                    for sk in sks:
                        if np.all(v[sk]>0):
                            plt.plot(x[sk],y[sk], linewidth=3, color=c)
                    plt.plot(x[v>0], y[v>0],'o',markersize=8, markerfacecolor=c, markeredgecolor='k',markeredgewidth=2)
                    plt.plot(x[v>1], y[v>1],'o',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)

                if draw_bbox:
                    [bbox_x, bbox_y, bbox_w, bbox_h] = ann['bbox']
                    poly = [[bbox_x, bbox_y], [bbox_x, bbox_y+bbox_h], [bbox_x+bbox_w, bbox_y+bbox_h], [bbox_x+bbox_w, bbox_y]]
                    np_poly = np.array(poly).reshape((4,2))
                    polygons.append(Polygon(np_poly))
                    color.append(c)

            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)
            ax.add_collection(p)
            p = PatchCollection(polygons, facecolor='none', edgecolors=color, linewidths=2)
            ax.add_collection(p)
        elif datasetType == 'captions':
            for ann in anns:
                print(ann['caption'])

    def loadRes(self, resFile):
        """
        Load result file and return a result api object.
        :param   resFile (str)     : file name of result file
        :return: res (obj)         : result api object
        """
        res = COCO()
        res.dataset['images'] = [img for img in self.dataset['images']]

        print('Loading and preparing results...')
        tic = time.time()
        if type(resFile) == str or (PYTHON_VERSION == 2 and type(resFile) == unicode):
            anns = json.load(open(resFile))
        elif type(resFile) == np.ndarray:
            anns = self.loadNumpyAnnotations(resFile)
        else:
            anns = resFile
        assert type(anns) == list, 'results in not an array of objects'
        annsImgIds = [ann['image_id'] for ann in anns]
        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \
               'Results do not correspond to current coco set'
        if 'caption' in anns[0]:
            imgIds = set([img['id'] for img in res.dataset['images']]) & set([ann['image_id'] for ann in anns])
            res.dataset['images'] = [img for img in res.dataset['images'] if img['id'] in imgIds]
            for id, ann in enumerate(anns):
                ann['id'] = id+1
        elif 'bbox' in anns[0] and not anns[0]['bbox'] == []:
            res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])
            for id, ann in enumerate(anns):
                bb = ann['bbox']
                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]
                if not 'segmentation' in ann:
                    ann['segmentation'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]
                ann['area'] = bb[2]*bb[3]
                ann['id'] = id+1
                ann['iscrowd'] = 0
        elif 'segmentation' in anns[0]:
            res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])
            for id, ann in enumerate(anns):
                # now only support compressed RLE format as segmentation results
                ann['area'] = maskUtils.area(ann['segmentation'])
                if not 'bbox' in ann:
                    ann['bbox'] = maskUtils.toBbox(ann['segmentation'])
                ann['id'] = id+1
                ann['iscrowd'] = 0
        elif 'keypoints' in anns[0]:
            res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])
            for id, ann in enumerate(anns):
                s = ann['keypoints']
                x = s[0::3]
                y = s[1::3]
                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)
                ann['area'] = (x1-x0)*(y1-y0)
                ann['id'] = id + 1
                ann['bbox'] = [x0,y0,x1-x0,y1-y0]
        print('DONE (t={:0.2f}s)'.format(time.time()- tic))

        res.dataset['annotations'] = anns
        res.createIndex()
        return res

    def download(self, tarDir = None, imgIds = [] ):
        '''
        Download COCO images from mscoco.org server.
        :param tarDir (str): COCO results directory name
               imgIds (list): images to be downloaded
        :return:
        '''
        if tarDir is None:
            print('Please specify target directory')
            return -1
        if len(imgIds) == 0:
            imgs = self.imgs.values()
        else:
            imgs = self.loadImgs(imgIds)
        N = len(imgs)
        if not os.path.exists(tarDir):
            os.makedirs(tarDir)
        for i, img in enumerate(imgs):
            tic = time.time()
            fname = os.path.join(tarDir, img['file_name'])
            if not os.path.exists(fname):
                urlretrieve(img['coco_url'], fname)
            print('downloaded {}/{} images (t={:0.1f}s)'.format(i, N, time.time()- tic))

    def loadNumpyAnnotations(self, data):
        """
        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}
        :param  data (numpy.ndarray)
        :return: annotations (python nested list)
        """
        print('Converting ndarray to lists...')
        assert(type(data) == np.ndarray)
        print(data.shape)
        assert(data.shape[1] == 7)
        N = data.shape[0]
        ann = []
        for i in range(N):
            if i % 1000000 == 0:
                print('{}/{}'.format(i,N))
            ann += [{
                'image_id'  : int(data[i, 0]),
                'bbox'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],
                'score' : data[i, 5],
                'category_id': int(data[i, 6]),
                }]
        return ann

    def annToRLE(self, ann):
        """
        Convert annotation which can be polygons, uncompressed RLE to RLE.
        :return: binary mask (numpy 2D array)
        """
        t = self.imgs[ann['image_id']]
        h, w = t['height'], t['width']
        segm = ann['segmentation']
        if type(segm) == list:
            # polygon -- a single object might consist of multiple parts
            # we merge all parts into one mask rle code
            rles = maskUtils.frPyObjects(segm, h, w)
            rle = maskUtils.merge(rles)
        elif type(segm['counts']) == list:
            # uncompressed RLE
            rle = maskUtils.frPyObjects(segm, h, w)
        else:
            # rle
            rle = ann['segmentation']
        return rle

    def annToMask(self, ann):
        """
        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.
        :return: binary mask (numpy 2D array)
        """
        rle = self.annToRLE(ann)
        m = maskUtils.decode(rle)
        return

# Validation dataset

dataset_val = CocoDataset()
coco = dataset_val.load_coco_val()
dataset_val.prepare()
print("Running COCO evaluation on validation images.")
#evaluate_coco(model, dataset_val, coco, "bbox")

image_ids = dataset_val.image_ids

limit = 0
if limit:
        image_ids = image_ids[:limit]

# Get corresponding COCO image IDs.
coco_image_ids = [dataset_val.image_info[id]["id"] for id in image_ids]

t_prediction = 0
t_start = time.time()

results = []

for i, image_id in enumerate(image_ids):
    # Load image
    image = dataset_val.load_image(image_id)

    # Run detection
    t = time.time()
    r = model.detect([image], verbose=0)[0]
    t_prediction += (time.time() - t)

    # Convert results to COCO format
    # Cast masks to uint8 because COCO tools errors out on bool
    image_results = build_coco_results(dataset_val, coco_image_ids[i:i + 1],
                                        r["rois"], r["class_ids"],
                                        r["scores"],
                                        r["masks"].astype(np.uint8))
    results.extend(image_results)

"""Mean IoU"""

num_results = 0
sum_iou_score = 0

for result in results:
  iou_score = result['score']
  sum_iou_score = sum_iou_score + iou_score
  num_results = num_results + 1

mean_iou = sum_iou_score/num_results

print('Mean IoU Score across all images and classes: ', mean_iou)

"""Mean Classwise IoU"""

num_abdomen_results = num_wing_results = num_leg_results = num_thorax_results = num_head_results = num_palps_results = num_proboscis_results = num_antennae_results = 0
sum_abdomen_iou_score = sum_wing_iou_score = sum_leg_iou_score = sum_thorax_iou_score = sum_head_iou_score = sum_palps_iou_score = sum_proboscis_iou_score = sum_antennae_iou_score = 0

for result in results:
  if result['category_id'] == 0: #abdomen
    abdomen_iou_score = result['score']
    sum_abdomen_iou_score = sum_abdomen_iou_score + abdomen_iou_score
    num_abdomen_results = num_abdomen_results + 1

  elif result['category_id'] == 1: #wing
    wing_iou_score = result['score']
    sum_wing_iou_score = sum_wing_iou_score + wing_iou_score
    num_wing_results = num_wing_results + 1

  elif result['category_id'] == 2: #leg
    leg_iou_score = result['score']
    sum_leg_iou_score = sum_leg_iou_score + leg_iou_score
    num_leg_results = num_leg_results + 1

  elif result['category_id'] == 3: #thorax
    thorax_iou_score = result['score']
    sum_thorax_iou_score = sum_thorax_iou_score + thorax_iou_score
    num_thorax_results = num_thorax_results + 1

  elif result['category_id'] == 4: #head
    head_iou_score = result['score']
    sum_head_iou_score = sum_head_iou_score + head_iou_score
    num_head_results = num_head_results + 1

  elif result['category_id'] == 5: #palps
    palps_iou_score = result['score']
    sum_palps_iou_score = sum_palps_iou_score + palps_iou_score
    num_palps_results = num_palps_results + 1

  elif result['category_id'] == 6: #proboscis
    proboscis_iou_score = result['score']
    sum_proboscis_iou_score = sum_proboscis_iou_score + proboscis_iou_score
    num_proboscis_results = num_proboscis_results + 1

  elif result['category_id'] == 7: #antennae
    antennae_iou_score = result['score']
    sum_antennae_iou_score = sum_antennae_iou_score + antennae_iou_score
    num_antennae_results = num_antennae_results + 1

if num_abdomen_results != 0:
  mean_iou_abdomen = sum_abdomen_iou_score/num_abdomen_results
else:
  mean_iou_abdomen = 0

if num_wing_results != 0:
  mean_iou_wing = sum_wing_iou_score/num_wing_results
else:
  mean_iou_wing = 0

if num_leg_results != 0:
  mean_iou_leg = sum_leg_iou_score/num_leg_results
else:
  mean_iou_leg = 0

if num_thorax_results != 0:
  mean_iou_thorax = sum_thorax_iou_score/num_thorax_results
else:
  mean_iou_thorax = 0

if num_head_results != 0:
  mean_iou_head = sum_head_iou_score/num_head_results
else:
  mean_iou_head = 0

if num_palps_results != 0:
  mean_iou_palps = sum_palps_iou_score/num_palps_results
else:
  mean_iou_palps = 0

if num_proboscis_results != 0:
  mean_iou_proboscis = sum_proboscis_iou_score/num_proboscis_results
else:
  mean_iou_proboscis = 0

if num_antennae_results != 0:
  mean_iou_antennae = sum_antennae_iou_score/num_antennae_results
else:
  mean_iou_antennae = 0

print('Mean IoU scores for each of the categories are as follows: \n')
print('abdomen: ', mean_iou_abdomen)
print('wing: ', mean_iou_wing)
print('leg: ', mean_iou_leg)
print('thorax: ', mean_iou_thorax)
print('head: ', mean_iou_head)
print('palps: ', mean_iou_palps)
print('proboscis: ', mean_iou_proboscis)
print('antennae: ', mean_iou_antennae)
